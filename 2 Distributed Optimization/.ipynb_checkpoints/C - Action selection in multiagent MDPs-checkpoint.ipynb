{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c113ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7048a27",
   "metadata": {},
   "source": [
    "## 2.2 Action selection in multiagent MDPs\n",
    "In the last section we did a review of MDPs. Unsuprisingly, there are a lot of ways to improve on the basic idea. \n",
    "\n",
    "One option is exploit the independence between states, using what are called 'factored' state spaces. With a factored model we can simplify the process of calculating the transition probabilities. E.g., if you have two factors $X$ and $Y$ where the transition between states in $X$ and states in $Y$ is indepenent given the action. We can also simply the value function, by instead approximating $V$ by some function of the factors. For example, a weighted sum (if the states were numeric). The weights can be found with a method called temporal difference learning.\n",
    "\n",
    "Another option, which we will cover here, is to exploit the independence between different actions. We assume agents are responsible for different choices, and hence the action $a$ can be split into a set of sub-actions ($a_1$,$a_2$,...). Unfortunately, actions is exponential in the number of individual agents. We want a faster option.\n",
    "\n",
    "Say we know $Q$ already. Then we can recover the best action in state $s$ easily, as $\\text{arg}\\max_a Q(s,a)$. \n",
    "\n",
    "If we then assume that $Q$ breaks down into some $Q_i$ for each agent $i$, then this becomes\n",
    "$\\text{arg}\\max_a \\sum Q_i(s,a)$.\n",
    "\n",
    "Finally, if we also assume that $Q_i$ only depends on *some* of the actions, then we might be able to use the variable elimination algorithm.\n",
    "\n",
    "E.g.,\n",
    "\n",
    "$$\\max_{a_1} \\max_{a_2} \\max_{a_3} Q_1(s,a_1,a_2) + Q_2(s,a_2,a_3) + Q_3(s,a_3)$$\n",
    "\n",
    "Can become:\n",
    "\n",
    "$$\\max_{a_1} \\max_{a_2} Q_1(s,a_1,a_2) + \\max_{a_3} \\big[ Q_2(s,a_2,a_3) + Q_3(s,a_3) \\big]$$\n",
    "\n",
    "An an example let's consider the following problem. We have multiple agents. Each agent has some position. Each agent has their own utility function, BUT each agent will get a bonus if it is the same as the one after it. For simplicity say each agent gets to choose it's position. This basically means our $Q_i$ only depends on the actions of agents $i$ and $i+1$. We can then go forward through every agent and calculate the max. These can be viewed as messages, each containing the new estimate of the values of the receiving agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db6bde",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "number_of_agents = 10\n",
    "number_of_positions = 5\n",
    "alignment_bonus = 0.3\n",
    "utilities = np.random.rand(number_of_agents,number_of_positions)\n",
    "message = utilities[0] # a message gives your new values, so start just with the utilities of the first agent\n",
    "decisions = [] # what do you do give the agent in front\n",
    "for current_agent in range(number_of_agents)[:-1]:\n",
    "    new_message = np.zeros(number_of_positions)\n",
    "    my_choices = np.zeros(number_of_positions).astype(int)\n",
    "    for option in range(number_of_positions): # for each option the next agent has available\n",
    "        u = message.copy()\n",
    "        u[option] += alignment_bonus # add a bonus if the next agent is the same as you\n",
    "        new_message[option] = np.max(u) + utilities[current_agent+1,option]\n",
    "        my_choices[option] = np.argmax(u)\n",
    "    message = new_message\n",
    "    decisions.append(my_choices)\n",
    "\n",
    "# finally add the last agents choice, which isn't conditional\n",
    "decisions.append(np.argmax(message))\n",
    "\n",
    "positions = np.random.choice(number_of_positions,number_of_agents)\n",
    "position_history = [positions.copy()]\n",
    "\n",
    "# do a simulation, running until no more agents are moving.\n",
    "any_change = True\n",
    "while any_change:\n",
    "    any_change = False\n",
    "    for agent in np.random.choice(number_of_agents,number_of_agents,replace=False):\n",
    "        if(agent==number_of_agents-1):\n",
    "            new_position = decisions[agent]\n",
    "        else:\n",
    "            new_position = decisions[agent][positions[agent+1]]\n",
    "            \n",
    "        if(not positions[agent]==new_position):\n",
    "            positions[agent] = new_position\n",
    "            any_change = True\n",
    "            \n",
    "    if any_change:\n",
    "        position_history.append(positions.copy())\n",
    "\n",
    "fig, axs = plt.subplots(1, len(position_history), figsize=(15, 15))\n",
    "for i in range(len(position_history)):\n",
    "    axs[i].imshow(utilities,origin=\"lower\",cmap=\"gray\")\n",
    "    axs[i].scatter(position_history[i],np.arange(number_of_agents),s=350,color=\"red\")\n",
    "    axs[i].axis(\"off\")\n",
    "    axs[i].set_title(f'Step {i + 1}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b2f70d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
