{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64852d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6a985f",
   "metadata": {},
   "source": [
    "# Review of MDP's\n",
    "\n",
    "### Setup\n",
    "In a markov decision problem we have a set of states $S$, a set of actions $A$, a transition probability function $P$, and a reward function $R$. An agent takes actions, which then lead to transitions in state. Each state-action pair is associated with a reward. The transition function outputs the probability of arrivingt in a given state as a result of being in a given state and applying a given action.\n",
    "\n",
    "We want to find an optimal policy $\\pi$ of what action to apply in each state. To do this we use the following, called the Bellman Equations:\n",
    "\n",
    "$$Q^\\pi(s,a) = r(s,a) + \\beta \\sum_\\hat{s} P(\\hat{s} | s, a) V^\\pi(\\hat{s}) $$\n",
    "\n",
    "$$V^\\pi(s) = Q^\\pi(s,\\pi(s)) $$\n",
    "\n",
    "Here $V^\\pi(s)$ is called the value function, $Q^\\pi$ is called the state-action value function, and $\\beta$ is a discount between 0 and 1. The second equation says the value of being in $s$ is given by the state-action value of being in state $s$ and taking action $\\pi(s)$. The first equation says that the value of being in a state and applying an action is the immediate reward plus the expectation over the value of your future state, $\\hat{s}$.\n",
    "\n",
    "Given values for $Q$ we can work out the value of $V$ *if* you have the optimal policy:\n",
    "\n",
    "$$V(s) = \\max_a Q(s,a) $$\n",
    "\n",
    "This means we can then remove $Q$ and just state $V(s)$ as:\n",
    "\n",
    "$$V(s) = \\max_a r(s,a) + \\beta \\sum_\\hat{s} P(\\hat{s} | s, a) V(\\hat{s}) $$\n",
    "\n",
    "This is what will be true for the optimal value of $\\pi$.\n",
    "\n",
    "We can turn this function into a contraction mapping $T$:\n",
    "\n",
    "$$T(V(s)) = \\max_a r(s,a) + \\beta \\sum_\\hat{s} P(\\hat{s} | s, a) V(\\hat{s}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506f387",
   "metadata": {},
   "source": [
    "### Contraction mappings\n",
    "\n",
    "This is a type of function (going from $V$ to $V$) which is called a contraction mapping. A function $T$ on a metric space $X$ is a contraction mapping if \n",
    "\n",
    "$$d(T(x),T(y))\\leq \\alpha d(x,y) \\text{ for all } x,y \\in X$$\n",
    "\n",
    "and $0\\geq\\alpha<1$ \n",
    "\n",
    "There is then a single point $x$ for which $T(x)=x$.\n",
    "\n",
    "Proof: consider there are two points $x$ and $y$ for which $T(x)=x$ and $T(y)=y$. By definition: \n",
    "\n",
    "$$d(T(x),T(y))\\leq \\alpha d(x,y)$$ \n",
    "\n",
    "which means \n",
    "\n",
    "$$d(x,y)\\leq \\alpha d(x,y)$$\n",
    "\n",
    "$$(1 - \\alpha) d(x,y) \\leq 0$$\n",
    "\n",
    "And as $0\\geq\\alpha<1$:\n",
    "\n",
    "$$d(x,y) = 0$$\n",
    "\n",
    "Secondly, the function $T$ converges to this point $x$. Consider if I start off in position $y$. By definition:\n",
    "\n",
    "$$d(T(x),T(y))\\leq \\alpha d(x,y)$$ \n",
    "\n",
    "Which means:\n",
    "\n",
    "$$d(x,T(y))\\leq \\alpha d(x,y)$$ \n",
    "\n",
    "So $T(y)$ is closer to $x$ than $y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9411ba5",
   "metadata": {},
   "source": [
    "### Showing the bellman equation is a contraction:\n",
    "\n",
    "We will use the suprenum norm as our distance, which takes the maximum absolute difference across all dimensions (states in this case).\n",
    "\n",
    "To get the absolute difference for a state in the case of the bellman function we can see:\n",
    "\n",
    "$$|(T(V_1(s)),T(V_2(s))|=\\max_a \\left[ r(s,a) + \\beta \\sum_\\hat{s} P(\\hat{s} | s, a) V_1(\\hat{s}) \\right] - \\max_a \\left[ r(s,a) + \\beta \\sum_\\hat{s} P(\\hat{s} | s, a) V_2(\\hat{s}) \\right] $$\n",
    "\n",
    "Note $\\max (a+b) \\leq \\max(a) + \\max(b)$. Therefore we can rewrite:\n",
    "\n",
    "$$|(T(V_1(s)),T(V_2(s))|\\leq\\left[\\max_a r(s,a) + \\max_a \\beta \\sum_\\hat{s} P(\\hat{s} | s, a) V_1(\\hat{s}) \\right] - \\left[ \\max_a r(s,a) + \\max_a \\beta \\sum_\\hat{s} P(\\hat{s} | s, a) V_2(\\hat{s}) \\right] $$\n",
    "\n",
    "Cancelling out the $\\max_a r(s,a)$, pulling $\\beta$ in front:\n",
    "\n",
    "$$|(T(V_1(s)),T(V_2(s))|\\leq \\beta \\left[\\max_a \\sum_\\hat{s} P(\\hat{s} | s, a) V_1(\\hat{s}) - \\max_a \\sum_\\hat{s} P(\\hat{s} | s, a) V_2(\\hat{s}) \\right] $$\n",
    "\n",
    "Finally, we use the inequality $|\\max_x f(x)| - |\\max_x g(x)|\\leq \\max_x|f(x)-g(x)|$. Why? Say $\\max_x g(x)$ is bigger than $\\max_x f(x)$. Let the value of $x$ at $\\max_x g(x)$ be denoted $x_g$. By definition of maximum $\\max_x f(x)\\geq f(x_g)$. And the only way to shrink the absolute difference would be if $f(x_g)$ was *greater* than $\\max_x f(x)$, which can't happen. So:\n",
    "\n",
    "$$|(T(V_1(s)),T(V_2(s))|\\leq \\beta \\max_a \\sum_\\hat{s} P(\\hat{s} | s, a)|V_1(\\hat{s}) - V_2(\\hat{s})| $$\n",
    "\n",
    "Say the suprenum norm $d(V_1,V_2)=l$.\n",
    "\n",
    "Then at best $d(T(V_1),T(V_2))=\\beta l$. \n",
    "\n",
    "Why? If $l$ is the largest absolute difference, then all the values of $|V_1(\\hat{s})-V_2(\\hat{s})|$ are at most $l$. And the expectation over $P(\\hat{s} | s, a)$ is also then at most $l$. \n",
    "\n",
    "And as $\\beta$ is always less than 1 this means $d(T(V_1),T(V_2))<d(V_1,V_2)$. So the bellman update function above is a contraction mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa5e21",
   "metadata": {},
   "source": [
    "### Wrapping it together\n",
    "\n",
    "Because the bellman equation $V(s) = \\max_a r(s,a) + \\beta \\sum_\\hat{s} P(\\hat{s} | s, a) V(\\hat{s}) $ will be true at the optimum, and because $T$ is a contraction mapping, we have a proceedure to calculate the value function by simply appying $T$ again and again.\n",
    "\n",
    "Here is an example where you are playing go-karts and trying to pass someone else.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3d03aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"left_back\",\"right_back\",\"right_mid\",\"left_forward\",\"right_forward\",\"crashed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "bbdc729e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1224x432 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the state of a kart trying to pass:\n",
    "def plot_passing_state(current_state,ax):\n",
    "    road1 = patches.Rectangle((6, 0), 5, 19, linewidth=1, edgecolor='lightgray', facecolor='lightgray')\n",
    "    road2 = patches.Rectangle((0, 0), 5, 19, linewidth=1, edgecolor='lightgray', facecolor='lightgray')\n",
    "    ax.add_patch(road1)\n",
    "    ax.add_patch(road2)\n",
    "    car1 = patches.Rectangle((1, 7), 3, 5, linewidth=1, edgecolor='black', facecolor='black')\n",
    "    ax.add_patch(car1)\n",
    "    if(current_state==\"crashed\"):\n",
    "        crash = patches.RegularPolygon((2.5, 9.5), numVertices=6, radius=2,color=\"red\")\n",
    "        ax.add_patch(crash)\n",
    "    else:\n",
    "        car2_x = {\"left\":1,\"right\":7}[current_state.split(\"_\")[0]]\n",
    "        car2_y = {\"back\":1,\"mid\":7,\"forward\":13}[current_state.split(\"_\")[1]]\n",
    "        car2 = patches.Rectangle((car2_x, car2_y), 3, 5, linewidth=1, edgecolor='red', facecolor='red')\n",
    "        ax.add_patch(car2)\n",
    "    ax.set_xlim(0, 11)\n",
    "    ax.set_ylim(0, 19)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(current_state)\n",
    "\n",
    "fig, axes = plt.subplots(1,6,figsize=(17, 6))\n",
    "for s in range(len(states)):\n",
    "    plot_passing_state(states[s],axes[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "5bef82a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "actions = [\"left\",\"right\",\"forward\",\"back\",\"stay_put\"]\n",
    "\n",
    "# making the transition matrix for each action.\n",
    "# when you try going forward or backward you might go left or right instead. \n",
    "\n",
    "go_left_T_dict = {\n",
    "    \"left_back\":{\"crashed\":1}, # you drove off the road\n",
    "    \"right_back\":{\"left_back\":1},\n",
    "    \"right_mid\":{\"crashed\":1},\n",
    "    \"left_forward\":{\"left_back\":1}, # reset if you get to left_forward\n",
    "    \"right_forward\":{\"left_forward\":1},\n",
    "    \"crashed\":{\"left_back\":1} # crash so reset\n",
    "}\n",
    "\n",
    "go_right_T_dict = {\n",
    "    \"left_back\":{\"right_back\":1},\n",
    "    \"right_back\":{\"crashed\":1},\n",
    "    \"right_mid\":{\"crashed\":1},\n",
    "    \"left_forward\":{\"left_back\":1},\n",
    "    \"right_forward\":{\"crashed\":1},\n",
    "    \"crashed\":{\"left_back\":1}\n",
    "}\n",
    "\n",
    "go_forward_T_dict = {\n",
    "    \"left_back\":{\"crashed\":0.9,\"right_back\":0.1}, # chance you drive left + change you go forward & crash = 90%.  \n",
    "    \"right_back\":{\"right_mid\":0.8,\"left_back\":0.1,\"crashed\":0.1}, # aim forward, might go left / right. \n",
    "    \"right_mid\":{\"right_forward\":0.8,\"crashed\":0.2}, # chance of smaking into the left car / off the right\n",
    "    \"left_forward\":{\"left_back\":1},\n",
    "    \"right_forward\":{\"right_forward\":0.8,\"left_forward\":0.1,\"crashed\":0.1},\n",
    "    \"crashed\":{\"left_back\":1}\n",
    "}\n",
    "\n",
    "go_back_T_dict = {\n",
    "    \"left_back\":{\"left_back\":0.8,\"right_back\":0.1,\"crashed\":0.1},\n",
    "    \"right_back\":{\"right_back\":0.8,\"left_back\":0.1,\"crashed\":0.1},\n",
    "    \"right_mid\":{\"right_back\":0.8,\"crashed\":0.2},\n",
    "    \"left_forward\":{\"left_back\":1},\n",
    "    \"right_forward\":{\"right_mid\":0.8,\"left_forward\":0.1,\"crashed\":0.1},\n",
    "    \"crashed\":{\"left_back\":1}\n",
    "}\n",
    "\n",
    "stay_put_T_dict = {\n",
    "    \"left_back\":{\"left_back\":1},\n",
    "    \"right_back\":{\"right_back\":1},\n",
    "    \"right_mid\":{\"right_mid\":1},\n",
    "    \"left_forward\":{\"left_back\":1},\n",
    "    \"right_forward\":{\"right_forward\":1},\n",
    "    \"crashed\":{\"left_back\":1}\n",
    "}\n",
    "\n",
    "transition_matrix = np.zeros((5, 6, 6)) # 5 actions, 6 possible current states, 6 possible new states\n",
    "state_index = {state: i for i, state in enumerate(states)}\n",
    "all_action_transitions = [go_left_T_dict,go_right_T_dict,go_forward_T_dict,go_back_T_dict,stay_put_T_dict]\n",
    "for i,action_dict in enumerate(all_action_transitions):\n",
    "    for current_state, new_states in action_dict.items():\n",
    "        for next_state, prob in new_states.items():\n",
    "            transition_matrix[i, state_index[current_state], state_index[next_state]] = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ac642ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left_back right\n",
      "right_back forward\n",
      "right_mid forward\n",
      "left_forward left\n",
      "right_forward left\n",
      "crashed left\n"
     ]
    }
   ],
   "source": [
    "# big loss for crashing, some reward for winning. Note: here the reward doesn't depend on the action. \n",
    "reward = np.array([0,0,0,100,0,-100])\n",
    "beta = 0.99\n",
    "\n",
    "V = np.random.rand(6) # start with a random matrix\n",
    "for iteration in range(1000):\n",
    "    # get the value of being in each state and applying each action.\n",
    "    # this means summing over the value of the new states times the probability of those new states\n",
    "    expected_future_values = beta * np.sum(transition_matrix*V.reshape(1,1,-1),axis=2)\n",
    "    V = np.max(expected_future_values + reward.reshape(1,-1),axis=0)\n",
    "\n",
    "best_actions = [actions[a] for a in np.argmax(expected_future_values, axis=0)]\n",
    "for s in range(6):\n",
    "    print(states[s],best_actions[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0615ec98",
   "metadata": {},
   "source": [
    "This shows the actions have been learned correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c0ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
