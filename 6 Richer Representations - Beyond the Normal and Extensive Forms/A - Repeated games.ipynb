{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff9e94dd-d112-4310-b42c-fdc432a8b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9788036-59c0-4ebe-9e69-9807fcc02b2b",
   "metadata": {},
   "source": [
    "# 6. Richer Representations: Beyond the Normal and Extensive Forms\n",
    "\n",
    "There are several reasons we want to explore other forms of game. Firstly, so far we have assumed a lot of finite variables. The number of decisions / time is finite, the players are finite, and the actions have been finite. We may want to consider what happens for infinite agents, or for games which are repeated forever. Secondly, so far we have assumed that agents share a knowledge of eachother's payoffs. This is very unrealistic! Thirdly, we would like to find more compact ways of describing games, for the sake of efficiency.\n",
    "\n",
    "## 6.1 Repeated games\n",
    "\n",
    "Consider a game like the prisoner's dilemma, in normal form, which is played multiple times. In the case that agents have no information about previous games the answer is pretty trivial. However, if agents can see what happened before then things become more complicated.\n",
    "\n",
    "### 6.1.1 Finitely repeated games\n",
    "\n",
    "With a finitely repeated game we simply need a bigger table in the normal form (or tree in the extensive form) to capture the strategies and payoffs for both players. We assume that agents don't know what eachother will play, but find out later. One simple answer is just to play the same strategy at each game level, which we call a *stationary strategy*. But in general, the action can depend on what was played before. Consider the prisoner's dilemma repeated 100 times. If we explore the rational solutions we find that players should always defect every time. \n",
    "\n",
    "### 6.1.2 Infinitely repeated games\n",
    "\n",
    "If a game is repeated infinitely we get an infinite tree of decisions. In order to quantify the reward of being in a state we can consider the average reward over all games, or a discounted reward of future games (i.e., the agent cares more about the present, or the games might end at some random point). \n",
    "\n",
    "In infinitely repeated games there are strategies other than the stationary ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab20f567-e51a-47ad-a7f0-47300062bc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
