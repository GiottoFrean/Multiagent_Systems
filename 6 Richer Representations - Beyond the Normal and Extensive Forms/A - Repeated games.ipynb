{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff9e94dd-d112-4310-b42c-fdc432a8b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9788036-59c0-4ebe-9e69-9807fcc02b2b",
   "metadata": {},
   "source": [
    "# 6. Richer Representations: Beyond the Normal and Extensive Forms\n",
    "\n",
    "There are several reasons we want to explore other forms of game. Firstly, so far we have assumed a lot of finite variables. The number of decisions / time is finite, the players are finite, and the actions have been finite. We may want to consider what happens for infinite agents, or for games which are repeated forever. Secondly, so far we have assumed that agents share a knowledge of eachother's payoffs. This is very unrealistic! Thirdly, we would like to find more compact ways of describing games, for the sake of efficiency.\n",
    "\n",
    "## 6.1 Repeated games\n",
    "\n",
    "Consider a game like the prisoner's dilemma, in normal form, which is played multiple times. In the case that agents have no information about previous games the answer is pretty trivial. However, if agents can see what happened before then things become more complicated.\n",
    "\n",
    "### 6.1.1 Finitely repeated games\n",
    "\n",
    "With a finitely repeated game we simply need a bigger table in the normal form (or tree in the extensive form) to capture the strategies and payoffs for both players. We assume that agents don't know what eachother will play, but find out later. One simple answer is just to play the same strategy at each game level, which we call a *stationary strategy*. But in general, the action can depend on what was played before. Consider the prisoner's dilemma repeated 100 times. If we explore the rational solutions we find that players should always defect every time. \n",
    "\n",
    "### 6.1.2 Infinitely repeated games\n",
    "\n",
    "If a game is repeated infinitely we get an infinite tree of decisions. In order to quantify the reward of being in a state we can consider the average reward over all games, or a discounted reward of future games (i.e., the agent cares more about the present, or the games might end at some random point). \n",
    "\n",
    "In infinitely repeated games there are strategies other than the stationary ones. For instance we have Tit-for-Tat (TfT) in which players start by cooperating and then repeat whatever their opponent's strategy was.\n",
    "\n",
    "Ideally we want to be able to calculate the equilibirum strategies in infinitely repeated games. A good place to start is **The Folk Theorem**, which states that the possible rewards in an equilibrium game that can be obtained by the players are exactly their rewards in the original game, as long as both are at least equal to the minmax values. \n",
    "\n",
    "Recall we define the minmax reward for player $1$ as\n",
    "\n",
    "$$v_1=\\text{min}_{s_2}\\text{max}_{s_1}u_1(s_1,s_2)$$\n",
    "\n",
    "The above corresponds to player 1 going AFTER player 2 in the two player single game (This is just using 2 players, but adding others is fine, the minimisation is taken over all other player's actions).\n",
    "\n",
    "In order to prove the theorem we need to introduce a couple of concepts. Below $r_i$ is the average reward for player $i$ in the infinite game.\n",
    "\n",
    "1. **Enforcability:** $r_i$ is enforcable if $r_i\\geq v_i$.\n",
    "2. **Feasibility:** $r_i$ is feasible if it can be a written combination of the rewards in $u_i$ (i.e., just a mixture where the weights sum to 1).\n",
    "\n",
    "The theorem is proved by first noting that no player will accept less than their minmax value at equilibrium, and then by constructing an equilibrium solution where all players go through a sequence of outcomes together, punishing people who deviate by playing minmax against them.\n",
    "\n",
    "**Part 1:**\n",
    "\n",
    "Consider that there is a player for whom $r_i<v_i$. Then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc59960-746c-4f4e-9198-5c8834952bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
