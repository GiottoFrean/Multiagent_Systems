{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1761c18f-6eef-4e3f-a979-b7b6c10a8e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b66da4-4d19-42eb-9333-a2cf2d4eade7",
   "metadata": {},
   "source": [
    "## 6.2 Stochastic games\n",
    "\n",
    "Stochastic games are those in which players will repeatedly play a number of games in sequence.\n",
    "\n",
    "### 6.2.1 Definition\n",
    "\n",
    "We have a finite set of games, a finite set of players, a finite set of actions available to each player, transition probabilities, and a payoff function for each game. To get the overall payoff you can use average or discounted reward.\n",
    "\n",
    "### 6.2.2 Strategies and equilibria\n",
    "\n",
    "A player needs to decide what to do given the history of all previous games. We can restrict this in several ways:\n",
    "\n",
    "**Behavioural strategies**\n",
    "\n",
    "We can limit the players to choosing an action for each game independently. I.e., the action I choose for the nth game is based on the history of all games up to n. I don't decide at the start what actions to play at the start. The behavioural strategy is then just a probability of taking each possible action for the current game, based on the history so far.\n",
    "\n",
    "**Markov strategies**\n",
    "\n",
    "This can be further restricted by making each action only dependent on the game that was just played. In this way - Markov.\n",
    "\n",
    "**Stationary strategies**\n",
    "\n",
    "Finally, we can remove the time component by saying that given the last game that was played each agents next strategy is the same. In the pure Markov case you are allowed to have a different strategy for the same game at time $a$ and $b$ even if the previous game history is the same.\n",
    "\n",
    "There are a few theorems about the equilibria in a stochastic game:\n",
    "\n",
    "**Theorem:** Every discounted-reward stochastic game has a Markov perfect equilibrium.\n",
    "\n",
    "A Markov perfect equilibrium means that every player has a Markov strategy, and it is a nash equilibrium regardless of the starting game.\n",
    "\n",
    "There is not a similar theory for average-reward games, as the average reward might not converge (e.g., imagine a cycle). However, if we have a stochastic game such that every sub-game has a non-zero probability of being reached, regardless of the strategies, then the average-reward game does at least have a nash equilibrium. We call these irreducible stochastic games. This leads to this theorem:\n",
    "\n",
    "**Theorem:** For every two-player, general-sum irreducible stochastic game, and every feasible outcome with a payoff vector $r$ that provides to each player at least their minmax value, then $r$ is the payoff vector of a nash equilibrium. This is true for average-reward games, and games with a large enough discount factor.\n",
    "\n",
    "### 6.2.3 Computing equilibria\n",
    "\n",
    "In a couple of cases computing the equilibria to a stochastic game is simple - either if there is only 1 player affecting the transition probabilities (this is just a MDP), or something called 'seperable reward state independent transition'. In general though you need to solve a nonlinear problem. \n",
    "\n",
    "Often a variant of value iteration is used, but this can get stuck in local optima. In order to do it we take all the combinations of history and all the combinations of actions.\n",
    "\n",
    "The standard Bellman formulation states:\n",
    "\n",
    "$$Q^\\pi(s,a) = r(s,a) + \\beta \\sum_\\hat{s} P(\\hat{s} | s, a) V^\\pi(\\hat{s}) $$\n",
    "\n",
    "$$V^\\pi(s) = Q^\\pi(s,\\pi(s)) $$\n",
    "\n",
    "We then take the maximum over $\\pi$ for the second equation and plug it into the first to get:\n",
    "\n",
    "$$V(s) = \\max_a r(s,a) + \\beta \\sum_\\hat{s} P(\\hat{s} | s, a) V(\\hat{s}) $$\n",
    "\n",
    "This function is true at the optimum, and is a contraction mapping so we can find that point by iterating in the value iteration manner.\n",
    "\n",
    "With a two player game we instead have two different $Q$, $r$, and $V$ functions, as well as different (possibly mixed) policies $\\pi_1$,$\\pi_2$ and different actions:\n",
    "\n",
    "$$Q_1^\\pi(s,a_1,a_2) = r_1(s,a_1,a_2) + \\beta \\sum_\\hat{s} P(\\hat{s} | s, a_1, a_2) V_1^{\\pi_1,\\pi_2}(\\hat{s}) $$\n",
    "\n",
    "$$Q_2^\\pi(s,a_1,a_2) = r_2(s,a_1,a_2) + \\beta \\sum_\\hat{s} P(\\hat{s} | s, a_1, a_2) V_2^{\\pi_1,\\pi_2}(\\hat{s}) $$\n",
    "\n",
    "If we know the values for $Q_1$ and $Q_2$ we can work out the nash equilibria policies $\\pi_1$ and $\\pi_2$. In the pure case this means a single action per state, but they could be mixed. The value function is then taken as the expected value given the current state and both policies. \n",
    "\n",
    "This produces quite an easy computation. We just update the Q values for a given state, compute the nash equilibria, then use that to update the value of that state.\n",
    "\n",
    "The problem (other than a lack of convergence) is that we are forcing our actors to make the best decision in *each* game rather than over all the games. So any nash equilibria over the whole space are possibly ignored. \n",
    "\n",
    "**Example**\n",
    "\n",
    "Lets say we are both farming some land together. We can both either farm responsibly (C) or farm to maximise immediate revenue (D). The reward we get from farming depends on the state of the land, which in turn depends on what we do. The land can be healthy (H) or eroded (E). In both states the game is a prisoner's dilemma, but in the latter the payoffs are worst:\n",
    "\n",
    "*Healthy*\n",
    "\n",
    "$\n",
    "\\begin{array}{c|ccc}\n",
    "\\text{} & C & D \\\\\n",
    "\\hline\n",
    "\\text{C} & 4,4 & 1,5 \\\\\n",
    "\\text{D} & 5,1 & 2,2 \\\\\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "*Eroded*\n",
    "\n",
    "$\n",
    "\\begin{array}{c|ccc}\n",
    "\\text{} & C & D \\\\\n",
    "\\hline\n",
    "\\text{C} & 2,2 & 0,3 \\\\\n",
    "\\text{D} & 3,0 & 1,1 \\\\\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "If the land is healthy and both players cooperate it will stay healthy with 90% probability, otherwise eroding with 50% probability. \n",
    "If the land is eroded and both players cooperate it will get better with 30% probability, otherwise remaining eroded.\n",
    "\n",
    "The state is just the last game that was played and what the players did, followed by the current game in play. E.g., HCCH means the land was healthy, both players cooperated, and now the land is healthy for this game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "602f5883-582d-42c3-bfc1-615e566c142b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states ['HCCH', 'HCCE', 'HCDH', 'HCDE', 'HDCH', 'HDCE', 'HDDH', 'HDDE', 'ECCH', 'ECCE', 'ECDH', 'ECDE', 'EDCH', 'EDCE', 'EDDH', 'EDDE']\n"
     ]
    }
   ],
   "source": [
    "states = [\"\".join(s) for s in list(product([\"H\",\"E\"],[\"C\",\"D\"],[\"C\",\"D\"],[\"H\",\"E\"]))]\n",
    "r1 = []\n",
    "r2 = []\n",
    "for state in states:\n",
    "    if state[-1]==\"H\":\n",
    "        r1.append([[4,1],[5,2]]) # CC, CD, DC, DD\n",
    "        r2.append([[4,5],[1,2]])\n",
    "    else:\n",
    "        r1.append([[2,0],[3,1]])\n",
    "        r2.append([[2,3],[0,1]])\n",
    "r1 = np.array(r1)\n",
    "r2 = np.array(r2)\n",
    "transitionMatrix = np.zeros((len(states),2,2,len(states))) # current state, action 1, action 2, new state\n",
    "for s in range(len(states)):\n",
    "    for a1 in range(2):\n",
    "        for a2 in range(2):\n",
    "            healthyOutcome = states[s][-1] + (\"C\" if a1==0 else \"D\") + (\"C\" if a2==0 else \"D\") + \"H\"\n",
    "            erodedOutcome = states[s][-1] + (\"C\" if a1==0 else \"D\") + (\"C\" if a2==0 else \"D\") + \"E\"\n",
    "            if a1==0 and a2==0: # CC\n",
    "                if states[s][-1]==\"H\":\n",
    "                    transitionMatrix[s,a1,a2,states.index(healthyOutcome)]=0.9\n",
    "                    transitionMatrix[s,a1,a2,states.index(erodedOutcome)]=0.1\n",
    "                else:\n",
    "                    transitionMatrix[s,a1,a2,states.index(healthyOutcome)]=0.3\n",
    "                    transitionMatrix[s,a1,a2,states.index(erodedOutcome)]=0.7\n",
    "            else:\n",
    "                if states[s][-1]==\"H\":\n",
    "                    transitionMatrix[s,a1,a2,states.index(healthyOutcome)]=0.5\n",
    "                    transitionMatrix[s,a1,a2,states.index(erodedOutcome)]=0.5\n",
    "                else:\n",
    "                    transitionMatrix[s,a1,a2,states.index(healthyOutcome)]=0.0\n",
    "                    transitionMatrix[s,a1,a2,states.index(erodedOutcome)]=1.0\n",
    "\n",
    "print(\"states\",states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ccc508-8d56-46e2-a1bd-f1717d2c6e28",
   "metadata": {},
   "source": [
    "Now calculating Q, getting nash strategies, updating v..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bf76622c-70c7-4b30-b8f2-0455baa6c219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early!\n"
     ]
    }
   ],
   "source": [
    "# Warning! Might take a while...\n",
    "discount = 0.99\n",
    "v1 = np.random.rand(len(states))\n",
    "v2 = np.random.rand(len(states))\n",
    "for iteration in range(10000):\n",
    "    old_v1 = v1.copy()\n",
    "    old_v2 = v2.copy()\n",
    "    for s in range(len(states)):\n",
    "        q1 = r1[s] + discount*np.sum(transitionMatrix[s]*v1.reshape(1,1,-1),axis=2)\n",
    "        q2 = r2[s] + discount*np.sum(transitionMatrix[s]*v2.reshape(1,1,-1),axis=2)\n",
    "        nash_strategy1, nash_strategy2 = utils.lemke_howson(q1,q2)\n",
    "        pCC = nash_strategy1[0]*nash_strategy2[0]\n",
    "        pCD = nash_strategy1[0]*nash_strategy2[1]\n",
    "        pDC = nash_strategy1[1]*nash_strategy2[0]\n",
    "        pDD = nash_strategy1[1]*nash_strategy2[1]\n",
    "        v1[s]=pCC*q1[0,0]+pCD*q1[0,1]+pDC*q1[1,0]+pDD*q1[1,1]\n",
    "        v2[s]=pCC*q2[0,0]+pCD*q2[0,1]+pDC*q2[1,0]+pDD*q2[1,1]\n",
    "    if np.max(np.abs(v1 - old_v1)) < 1e-4 and np.max(np.abs(v2 - old_v2)) < 1e-4:\n",
    "        print(\"Stopping early!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f867edd4-32c3-4987-8f2d-0ea7e94c080f",
   "metadata": {},
   "source": [
    "We can see what the strategies are in each state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5126c443-aa26-4926-b983-f7cc169e5ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HCCH [0. 1.] [0. 1.]\n",
      "HCCE [0. 1.] [0. 1.]\n",
      "HCDH [0. 1.] [0. 1.]\n",
      "HCDE [0. 1.] [0. 1.]\n",
      "HDCH [0. 1.] [0. 1.]\n",
      "HDCE [0. 1.] [0. 1.]\n",
      "HDDH [0. 1.] [0. 1.]\n",
      "HDDE [0. 1.] [0. 1.]\n",
      "ECCH [0. 1.] [0. 1.]\n",
      "ECCE [0. 1.] [0. 1.]\n",
      "ECDH [0. 1.] [0. 1.]\n",
      "ECDE [0. 1.] [0. 1.]\n",
      "EDCH [0. 1.] [0. 1.]\n",
      "EDCE [0. 1.] [0. 1.]\n",
      "EDDH [0. 1.] [0. 1.]\n",
      "EDDE [0. 1.] [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "for s in range(len(states)):\n",
    "    q1 = r1[s] + discount*np.sum(transitionMatrix[s]*v1.reshape(1,1,-1),axis=2)\n",
    "    q2 = r2[s] + discount*np.sum(transitionMatrix[s]*v2.reshape(1,1,-1),axis=2)\n",
    "    nash_strategy1, nash_strategy2 = utils.lemke_howson(q1,q2)\n",
    "    print(states[s],nash_strategy1,nash_strategy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83cb587-c84a-47be-89f1-2c1976f9a08e",
   "metadata": {},
   "source": [
    "The strategy is to always defect, because this is always the best option in any subgame. We don't allow someone to use a strategy in one game as a threat in another and so on. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
