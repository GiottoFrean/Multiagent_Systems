{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2302a7b7-9e03-4d09-8780-a3fcb049a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04470a6-86d8-458c-adf0-22419673a94b",
   "metadata": {},
   "source": [
    "## 7.5 No-regret learning and universal consistency\n",
    "\n",
    "As discussed earlier (see 7-A) a learning rule is consistent if yields at least what you could get by having adopted a single pure strategy. We define a regret measure as the difference between the average reward you got and the average reward you would have got if you had played a pure strategy $s$ instead. A no-regret learning rule is one which guarantees no positive regret for any pure strategy $s$. We look at the pure strategies in the stage-game, but it doesn't matter if we looked at mixed strategies instead, as mixed strategies just use indifference between pure strategies anyway.\n",
    "\n",
    "An example of a no-regret strategy is the defect option in the Prisoner's Dilemma. Tit-for-tat is not no-regret, because if you play a defecting opponent you will have regret for having cooperated in the first round.\n",
    "\n",
    "A variety of no-regret learning algorithms exist, in particular these 2:\n",
    "\n",
    "**Regret matching:**\n",
    "\n",
    "At each timestep an action is chosen with probability equal to it's regret (i.e., how much better it would have been to play that action).\n",
    "\n",
    "Say we have a staghunt:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc}\n",
    "\\text{} & \\text{C} & \\text{D} \\\\\n",
    "\\hline\n",
    "\\text{C} & 4,4 & 0,1 \\\\\n",
    "\\text{D} & 1,0 & 1,1 \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c3bb151-3f2d-4b57-a56a-b8ceda15862a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true1_returns [1, 0, 1, 1, 0, 0, 3, 1, 0, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "true2_returns [1, 0, 1, 1, 0, 0, 3, 1, 0, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "# Payoff matrix for (Player1, Player2) given (action1, action2)\n",
    "# Actions: \"C\" = 0, \"D\" = 1\n",
    "payoff_matrix = {\n",
    "    (0, 0): (3, 3),  # C, C\n",
    "    (0, 1): (0, 1),  # C, D\n",
    "    (1, 0): (1, 0),  # D, C\n",
    "    (1, 1): (1, 1)   # D, D\n",
    "}\n",
    "\n",
    "# Track returns\n",
    "C1_returns, D1_returns, true1_returns = [], [], []\n",
    "C2_returns, D2_returns, true2_returns = [], [], []\n",
    "\n",
    "actions = [\"C\", \"D\"]\n",
    "\n",
    "for iteration in range(20):\n",
    "    if iteration == 0:\n",
    "        a1, a2 = 1, 1  # Start with (D, D)\n",
    "    else:\n",
    "        # Compute regrets for Player 1\n",
    "        C1_regret = np.mean(C1_returns) - np.mean(true1_returns)\n",
    "        D1_regret = np.mean(D1_returns) - np.mean(true1_returns)\n",
    "        probs1 = np.clip([C1_regret, D1_regret], 0, np.inf)+0.001\n",
    "        probs1 = probs1 / np.sum(probs1)\n",
    "\n",
    "        # Compute regrets for Player 2\n",
    "        C2_regret = np.mean(C2_returns) - np.mean(true2_returns)\n",
    "        D2_regret = np.mean(D2_returns) - np.mean(true2_returns)\n",
    "        probs2 = np.clip([C2_regret, D2_regret], 0, np.inf)+0.001\n",
    "        probs2 = probs2 / np.sum(probs2)\n",
    "\n",
    "        a1 = np.random.choice([0, 1], p=probs1)\n",
    "        a2 = np.random.choice([0, 1], p=probs2)\n",
    "\n",
    "    p1, p2 = payoff_matrix[(a1, a2)]\n",
    "    true1_returns.append(p1)\n",
    "    true2_returns.append(p2)\n",
    "    C1_returns.append(payoff_matrix[(0, a2)][0])\n",
    "    D1_returns.append(payoff_matrix[(1, a2)][0])\n",
    "    C2_returns.append(payoff_matrix[(a1, 0)][1])\n",
    "    D2_returns.append(payoff_matrix[(a1, 1)][1])\n",
    "\n",
    "print(\"true1_returns\",true1_returns)\n",
    "print(\"true2_returns\",true1_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e464c5f2-6938-4417-9e4f-c9142fa9fe06",
   "metadata": {},
   "source": [
    "There are a couple of times where D is randomly chosen, and a few times where it hurts to be CD, but it pretty quickly ends us with CC the whole time.\n",
    "\n",
    "**Smooth fictitious play:**\n",
    "\n",
    "With smooth fictitious play the agents stochastically respond rather than choosing the best action. For example, they might use a softmax over the actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20246a3-17b3-49b1-a4ab-358af1037a9a",
   "metadata": {},
   "source": [
    "## 7.6 Targeted learning\n",
    "\n",
    "Another approach recognises that doing well against all possible opponents (no-regret) is pretty strict. In some cases we may may care about performance against a targeted group vs the wider one differently. Targeted learning aims for some properties:\n",
    "1. Targeted optimality - the learning algorithm gives the best response to the target class\n",
    "2. Satefy - the learning algorithm at least gets the maxmin value against the wider group\n",
    "3. Autocompatibility - self-play is strictly Pareto efficient (best outcome possible)\n",
    "4. Efficiency - you shouldn't have to get to infinity to learn. There is some metric for this.\n",
    "\n",
    "The general algorithm for targeted learning looks like:\n",
    "1. Start by assuming the opponent is in the target set and play a best response. If it becomes clear this isn't the case, move on.\n",
    "2. Signal to the opponent whether they have the same learning strategy. If so, coordinate on Pareto outcome.\n",
    "3. Play to the security level\n",
    "\n",
    "This gets more complicated the more agents you add, unsuprisingly. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
