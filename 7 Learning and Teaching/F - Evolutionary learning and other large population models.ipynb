{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "616a1c8c-e8ac-4d21-a744-37cf77b745c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13161f83-0a39-4e82-b4a3-0485b6c8cae1",
   "metadata": {},
   "source": [
    "## 7.7 Evolutionary learning and other large-population models\n",
    "\n",
    "### 7.7.1 The replicator dynamic\n",
    "\n",
    "Say we have a population playing repeated normal games against one another. Let $\\theta_t(a)$ be the share of players who do action $a$ at time $t$. Then we can define the expected payoff to an agent at time $t$ as:\n",
    "\n",
    "$$u_t(a) = \\sum_{a^\\prime}\\theta_t(a^\\prime)u(a,a^\\prime)$$\n",
    "\n",
    "I.e., a weighted sum of the utility of that action, given what others will play. The share of the population taking a particular action will increase if it gives a higher utility than the avergage for the population, and will decrease otherwise. There are some formulas to back this up too, but basically the gradient of $\\theta_t(a)$ is $\\theta_t(a)(u_t(a)-u_t^*)$, where $u_t^*$ is the mean.\n",
    "\n",
    "One of the nice benefits of this model is you can kind-of view the ratio of the strategies as the mixed strategy of a single agent. \n",
    "\n",
    "As an example, consider the simple coordination game:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc}\n",
    "\\text{} & \\text{H} & \\text{T} \\\\\n",
    "\\hline\n",
    "\\text{H} & 1,1 & 0,0 \\\\\n",
    "\\text{T} & 0,0 & 1,1 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "But let's start out a slight majority doing T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "525f73bc-7e09-44c6-b528-fa0bfc75e397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_u 0.505 u_H 0.45 u_T 0.55 delta_H -0.025 delta_T 0.025\n",
      "mean_u 0.511 u_H 0.425 u_T 0.575 delta_H -0.037 delta_T 0.037\n",
      "mean_u 0.525 u_H 0.389 u_T 0.611 delta_H -0.053 delta_T 0.053\n",
      "mean_u 0.554 u_H 0.336 u_T 0.664 delta_H -0.073 delta_T 0.073\n",
      "mean_u 0.613 u_H 0.263 u_T 0.737 delta_H -0.092 delta_T 0.092\n",
      "mean_u 0.717 u_H 0.171 u_T 0.829 delta_H -0.093 delta_T 0.093\n",
      "mean_u 0.857 u_H 0.077 u_T 0.923 delta_H -0.06 delta_T 0.06\n",
      "mean_u 0.966 u_H 0.017 u_T 0.983 delta_H -0.016 delta_T 0.016\n",
      "mean_u 0.998 u_H 0.001 u_T 0.999 delta_H -0.001 delta_T 0.001\n",
      "mean_u 1.0 u_H 0.0 u_T 1.0 delta_H -0.0 delta_T 0.0\n"
     ]
    }
   ],
   "source": [
    "theta= np.array([0.45,0.55])\n",
    "for t in range(10):\n",
    "    u_H = theta[0]\n",
    "    u_T = theta[1]\n",
    "    mean_u = theta[0]*u_H + theta[1]*u_T\n",
    "    delta_H = theta[0]*(u_H-mean_u)\n",
    "    delta_T = theta[1]*(u_T-mean_u)\n",
    "    print(\"mean_u\",round(mean_u,3),\"u_H\",round(u_H,3),\"u_T\",round(u_T,3),\"delta_H\",round(delta_H,3),\"delta_T\",round(delta_T,3))\n",
    "    theta[0]+=delta_H\n",
    "    theta[1]+=delta_T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e3d56f-5c10-4c23-80c1-7feaf4f92344",
   "metadata": {},
   "source": [
    "As expected, everyone eventually just does T. This evolutionary model is not very different to the repeated-game model. There are several important concepts it's useful to define:\n",
    "1. A steady state. Like before, this is where the strategies don't change round-to-round.\n",
    "2. A stable steady state. Same as above, but if you change the state a little bit it doesn't evolve very far away.\n",
    "3. An asymptotically stable state. It evolves back to the steady state.\n",
    "\n",
    "The previous case is an example of 1 and 2, but not 3.\n",
    "\n",
    "Unsuprisingly, Nash equilibria also turn out to be steady states and vice-versa. Also, when 3 is the case it turns out to be a trembling-hand equilibrium too (perfect and isolated)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e354d1-505a-400a-8774-9829a0e495e7",
   "metadata": {},
   "source": [
    "### 7.7.2 Evolutionarily stable strategies (ESS)\n",
    "\n",
    "A strategy is ESS if it isn't 'invadable' by any other strategy. What this means is that in the replicator dynamic the payoff for the original strategy is greater than the new strategy, when the new is a small share. A weak ESS is where the payoffs are at least the same, not necessarily better (so the population won't shrink). \n",
    "\n",
    "Every ESS is a Nash equilibrium, but not every equilibrium strategy is an ESS, unless you have a symmetric 2-player game.\n",
    "\n",
    "Every ESS is unsuprisingly an asymptotically stable state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a73b74-a355-48a0-90d3-0a3fda24e274",
   "metadata": {},
   "source": [
    "### 7.7.3 Agent-based simulation and emergent convention\n",
    "\n",
    "While replicator dynamics are nice for their mathematical simplicity it is also worth noting that you can model each agent independently. A good rule for doing this, e.g., for modelling social norms, is to allow each agent to update their behaviour given their own history. 2 agents might meet eachother randomly every iteration, and switch to a new strategy if it looks better than their current one over the past n iterations. Following this kind of rule for a social convention model eventually leads to a payoff that is at least the maxmin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
