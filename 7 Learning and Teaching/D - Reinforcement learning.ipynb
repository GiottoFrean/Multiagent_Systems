{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b05149b3-1e15-49d1-9d1a-0c5ebe663124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linprog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98519a68-0031-4214-9b26-3c8b6793867e",
   "metadata": {},
   "source": [
    "## 7.4 Reinforcement learning\n",
    "\n",
    "### 7.4.1 Learning in unknown MDPs\n",
    "\n",
    "As covered in earlier notebooks (2-B) the main way for an agent to solve a known MDP is with value iteration. At each stage we calculate the $Q$ values (value of taking an action in a state), which is the immediate reward of that action plus a mixture over the value of future states, and then maximise this over the action to get $V$, the values in each state. But this requires knowing the transition probabilities and the rewards. What if we don't know either? It turns out that if you at least know the rewards at each iteration then without knowing the transition probabilities you can still converge to an accurate $Q$ (and hence $V$ too).\n",
    "\n",
    "The basic algorithm behind Q-learning is very simple. At each step take a random action in state $s_t$ and observe the new state $s_{t+1}$. Then update the value of $s_t$ as some mixture of the old value and the (reward + discounted future value). I.e.,\n",
    "\n",
    "\n",
    "$$Q_{t+1}(s_t,a_t) = (1-\\alpha_t)Q_t(s_t,a_t)+\\alpha_t(r(s_t,a_t)+\\beta V_t(s_{t+1})$$\n",
    "\n",
    "Note how the learning rate $\\alpha_t$ is time-dependent. It needs to sum to infinity over infinity, but the square has to sum to less than that. E.g., $\\frac{1}{t}$.\n",
    "\n",
    "Then we just update $V$ in the usual way by taking the maximum of $Q$ over the action. Here is a simple example. Say you have states A, B, and C. B is crap. A and C are both good. You can aim for any state. If you are already in the state you are aiming for you have a 50% chance of getting it vs getting B. If you are in another state you have a 75% chance. This means the optimal strategy is to flip between A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "91f55400-14db-471e-9e41-808f739f3d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_aim_A = np.array([\n",
    "    [0.5,0.5,0], # already in A\n",
    "    [0.75,0.25,0],\n",
    "    [0.75,0.25,0],\n",
    "])\n",
    "T_aim_B = np.array([\n",
    "    [0.25,0.75,0],\n",
    "    [0.25,0.50,0.25], # already in B (split the other options)\n",
    "    [0.0,0.75,0.25],\n",
    "])\n",
    "T_aim_C = np.array([\n",
    "    [0.0,0.25,0.75],\n",
    "    [0.0,0.25,0.75],\n",
    "    [0.0,0.5,0.5], # already in C\n",
    "])\n",
    "\n",
    "reward = np.array([5,0,6]) # same rewards-per-state everywhere, C is slightly better than A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb62eb12-ccef-4e75-9a95-ff2510d9f653",
   "metadata": {},
   "source": [
    "**True value iteration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b6914ba8-68e7-4898-8e72-ee00fa04d3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q [[41.25186521 40.12686521 42.77985032]\n",
      " [37.37686527 36.3861936  37.77985032]\n",
      " [43.37686529 41.26119364 42.52052199]]\n",
      "V [42.77985032 37.77985032 43.37686529]\n"
     ]
    }
   ],
   "source": [
    "Q = np.zeros((3,3)) # 3 states, 3 actions\n",
    "V = np.zeros(3)\n",
    "gamma = 0.9\n",
    "\n",
    "state = 0\n",
    "for t in range(100):\n",
    "    for state in range(3):\n",
    "        T_given_state = np.concatenate([T_aim_A[[state]],T_aim_B[[state]],T_aim_C[[state]]],axis=0)\n",
    "        future_values = T_given_state.dot(V)\n",
    "        Q[state] = reward[state] + gamma * future_values\n",
    "        V[state] = np.max(Q[state])\n",
    "\n",
    "print(\"Q\",Q)\n",
    "print(\"V\",V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b298c3f6-dde9-47e9-9454-de5d55695b83",
   "metadata": {},
   "source": [
    "**Q-learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4509b7b5-9f98-478d-be35-a0721ede5ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q [[41.09618421 39.78099005 42.63111412]\n",
      " [37.32365504 36.04446835 37.44804461]\n",
      " [43.17843919 40.52914154 42.02254613]]\n",
      "V [42.63111412 37.44804461 43.17843919]\n"
     ]
    }
   ],
   "source": [
    "Q = np.random.rand(3,3) # 3 states, 3 actions\n",
    "V = np.random.rand(3)\n",
    "alpha_func = lambda x: (1/(0.01*x+2)) # 0 -> 0.5, 100 -> 0.333, etc\n",
    "\n",
    "state = 0\n",
    "for t in range(10000):\n",
    "    alpha = alpha_func(t)\n",
    "    action = np.random.randint(3)\n",
    "    new_state = np.random.choice(3,p=[T_aim_A,T_aim_B,T_aim_C][action][state])\n",
    "    Q[state,action] = (1-alpha)*Q[state,action] + alpha * (reward[state] + gamma * V[new_state])\n",
    "    V[state] = np.max(Q[state])#\n",
    "    state = new_state\n",
    "\n",
    "print(\"Q\",Q)\n",
    "print(\"V\",V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed46ea2-36da-45dd-bcf0-44fbcb9e4854",
   "metadata": {},
   "source": [
    "Roughly the same...\n",
    "\n",
    "$Q$-learning will eventually converge to values matching the optimal policy, however it makes no guarantee about the rate of convergence. This is a bit of a problem as some agents might take on such a big loss early on that they can't make it up knowing the optimal policy going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2398640-bce2-420c-9ef2-559a10d32fce",
   "metadata": {},
   "source": [
    "### 7.4.2 Reinforcement learning in zero-sum\n",
    "Adding other players to the game comes with a number of challenges, as we have covered before. One option is to just ignore the other and use the above approach. In that scenario the other player is just a stochastic force that affects the transition probabilities. A more intelligent agent would instead model the other player. We can do this for $Q$-learning directly by modifying the formula:\n",
    "\n",
    "$$Q_{t+1}(s_t,a_t,o_t) = (1-\\alpha_t)Q_t(s_t,a_t,o_t)+\\alpha_t(r(s_t,a_t,o_t)+\\beta V_t(s_{t+1})$$\n",
    "\n",
    "Here we are just expanding $Q$ to include the other player's actions. This is all well and good, but how to get the value function? Well, in a zero-sum game the maxmin strategy for both players forms a nash equilibrium. Therefore we might say:\n",
    "\n",
    "$$V_{t+1}(s_t) = \\max_{a}\\min_{o}Q_{t}(s,a,o)$$\n",
    "\n",
    "One downside of this approach is that it means the agent may be unable to exploit when the opponent is using a sub-optimal strategy.\n",
    "\n",
    "To give an example of the algorithm, let's say that you are trying to run away from some opponent who is trying to catch you. You live on a 4 square long grid which loops around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d1be73af-100c-4bb9-b657-d6adda6609ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_________ [0.344 0.317 0.339]  value 6.397 \n",
      "A__B______ [1.000 -0.000 0.000] value 9.356 \n",
      "A_____B___ [-0.000 1.000 0.000] value 9.370 \n",
      "A________B [0.000 -0.000 1.000] value 9.372 \n",
      "B__A______ [-0.000 0.000 1.000] value 9.373 \n",
      "___X______ [0.322 0.348 0.330]  value 6.445 \n",
      "___A__B___ [1.000 0.000 -0.000] value 9.378 \n",
      "___A_____B [0.000 1.000 -0.000] value 9.358 \n",
      "B_____A___ [0.000 1.000 -0.000] value 9.387 \n",
      "___B__A___ [-0.000 0.000 1.000] value 9.366 \n",
      "______X___ [0.333 0.331 0.337]  value 6.459 \n",
      "______A__B [1.000 -0.000 0.000] value 9.360 \n",
      "B________A [1.000 0.000 -0.000] value 9.371 \n",
      "___B_____A [0.000 1.000 -0.000] value 9.370 \n",
      "______B__A [-0.000 0.000 1.000] value 9.362 \n",
      "_________X [0.331 0.331 0.339]  value 6.435 \n"
     ]
    }
   ],
   "source": [
    "# Define positions: 0=Left, 1=Center left, 2=Center right, 3 = Right\n",
    "positions = [0, 1, 2, 3]\n",
    "actions = [0, 1, 2]  # 0=Left, 1=Stay, 2=Right\n",
    "num_states = len(positions) ** 2\n",
    "num_actions = len(actions)\n",
    "\n",
    "# Q-table: Q1[state=(p1,p2), a1, a2]\n",
    "Q1 = np.ones((4, 4, num_actions, num_actions))\n",
    "\n",
    "# Value estimates and stochastic policies\n",
    "V1 = np.ones((4, 4))\n",
    "policy1 = np.ones((4, 4, num_actions))/num_actions  # probability distribution over actions\n",
    "\n",
    "reward1 = np.array([\n",
    "    [-1,  1,  1, 1],\n",
    "    [ 1, -1,  1, 1],\n",
    "    [ 1,  1,  -1, 1],\n",
    "    [ 1,  1,  1, -1],\n",
    "])\n",
    "reward2 = -reward1  # zero-sum\n",
    "\n",
    "# Exploration\n",
    "explore_prob = 0.5\n",
    "visits = np.zeros((4, 4), dtype=int)\n",
    "\n",
    "def alpha_func(n): \n",
    "    return 1.0 / (1.0 + 0.01*n)\n",
    "\n",
    "def sample_action(prob_dist, explore_prob):\n",
    "    if np.random.rand() < explore_prob:\n",
    "        return np.random.choice(actions)\n",
    "    return np.random.choice(actions, p=prob_dist)\n",
    "\n",
    "def transition(pos1, pos2, a1, a2):\n",
    "    new1 = (pos1 + (a1 - 1)) % 4\n",
    "    new2 = (pos2 + (a2 - 1)) % 4\n",
    "    return int(new1), int(new2)\n",
    "\n",
    "def solve_minimax(Q_matrix): # linear program to solve for the minimax mixed strategy\n",
    "    num_actions = Q_matrix.shape[0]\n",
    "    c = np.zeros(num_actions + 1)\n",
    "    c[-1] = -1\n",
    "    A_eq = np.zeros((1, num_actions + 1))\n",
    "    A_eq[0, :num_actions] = 1\n",
    "    b_eq = np.array([1.0])\n",
    "    A_ub = np.zeros((Q_matrix.shape[1], num_actions + 1))\n",
    "    b_ub = np.zeros(Q_matrix.shape[1])\n",
    "    for j in range(Q_matrix.shape[1]):\n",
    "        A_ub[j, :num_actions] = -Q_matrix[:, j]\n",
    "        A_ub[j, -1] = 1\n",
    "    bounds = [(0, 1)] * num_actions + [(None, None)]\n",
    "    res = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
    "    if res.success:\n",
    "        probs = res.x[:num_actions]\n",
    "        v = res.x[-1]\n",
    "        return probs, v\n",
    "    else:\n",
    "        return np.ones(num_actions) / num_actions, 0.0\n",
    "\n",
    "gamma = 0.9\n",
    "pos1, pos2 = 0, 0\n",
    "for t in range(10000):\n",
    "    alpha = alpha_func(visits[pos1, pos2])\n",
    "    visits[pos1, pos2] += 1\n",
    "    a1 = sample_action(policy1[pos1, pos2], explore_prob)\n",
    "    a2 = np.random.choice(actions) # player 2 can be random, doesn't matter!\n",
    "    new1, new2 = transition(pos1, pos2, a1, a2)\n",
    "    r1 = reward1[pos1, pos2]\n",
    "    future = r1 + gamma * V1[new1, new2]\n",
    "    Q1[pos1, pos2, a1, a2] = (1 - alpha) * Q1[pos1, pos2, a1, a2] + alpha * future\n",
    "    policy, v = solve_minimax(Q1[pos1, pos2])\n",
    "    policy1[pos1, pos2] = policy\n",
    "    V1[pos1, pos2] = v\n",
    "    pos1, pos2 = new1, new2\n",
    "\n",
    "\n",
    "def encode_state(pos1, pos2, size=4):\n",
    "    \"\"\"Encode a state as a string like A__B\"\"\"\n",
    "    symbols = [\"_\"] * size\n",
    "    symbols[pos1] = \"A\"\n",
    "    symbols[pos2] = \"B\" if not pos1==pos2 else \"X\"\n",
    "    return \"__\".join(symbols)\n",
    "    \n",
    "def format_policy(p):\n",
    "    # show each probability with 3 decimals\n",
    "    return \"[\" + \" \".join(f\"{x:5.3f}\" for x in p) + \"]\"\n",
    "\n",
    "for pos1 in positions:\n",
    "    for pos2 in positions:\n",
    "        state_code = encode_state(pos1, pos2)\n",
    "        policy_str = format_policy(policy1[pos1, pos2])\n",
    "        value_str = f\"{V1[pos1, pos2]:.3f}\"\n",
    "        # Print with fixed widths\n",
    "        print(f\"{state_code:10} {policy_str:20} value {value_str:6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bffd4c-fde4-43d6-b6d5-0aa8f4e4d2ec",
   "metadata": {},
   "source": [
    "In the above you are A and the other is B, with a clash marked by X. On the clash spots the strategy is to randomly choose whether to go left or right or stay put. Otherwise the strategy is to run away in the obvious direction.\n",
    "\n",
    "This algorithm will return the correct values in a zero-sum game eventually. There are algorithms which guarantee a weaker reward of maxmin - $\\epsilon$ for some $\\epsilon$, e.g., the R-max algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81ba6f-91d4-4529-ac51-34ba142f2129",
   "metadata": {},
   "source": [
    "### 7.4.3 Beyond zero-sum stochastic games\n",
    "\n",
    "$Q$-learning does not generalize to general-sum games. Althought well-defined the above algorithm isn't guaranteed to get to the maxmin strategy equilibrium, nor to the Nash equilibrium. There are algorithms for 'pure-coordination' games.\n",
    "\n",
    "### 7.4.4 Belief-based reinforcement learning\n",
    "\n",
    "It is possible to have models of the opponent's behaviour, as in Fictitious play. You need to add a belief function over the actions that the other player will take, and update it as you learn. There is some indication that these can converge to equilibrium in self-play, but nothing theoretical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
