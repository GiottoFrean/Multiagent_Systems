{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b1f17f-5876-4df0-96eb-3a16da3ee727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8706290c-3a4c-42cf-a022-dc23f4e9ef9f",
   "metadata": {},
   "source": [
    "## 7.2 Fititious play\n",
    "\n",
    "Fictitious play is a very simple learning rule. Like a lot of learning rules agents will maintain a model of the opponent's strategy. At each iteration this model is updated based on the decision the opponent makes, after the player updates their own strategy. Essentially players track the emprical distribution of their opponent's actions and respond accordingly. Note that agents here are basically unaware of their opponent's payoffs.\n",
    "\n",
    "For example, consider the (repeated) game of Rock-Paper-Scissors:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc}\n",
    "\\text{} & \\text{R} & \\text{P} & \\text{S} \\\\\n",
    "\\hline\n",
    "\\text{R} & 0,0 & -1,1 & 1,-1 \\\\\n",
    "\\text{P} & 1,-1 & 0,0 & -1,1 \\\\\n",
    "\\text{S} & -1,1 & 1,-1 & 0,0 \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3401d1f-7912-4a90-9299-8cddc0d578c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "  P1 dist: [2 1 1]\n",
      "  P2 dist: [1 2 1]\n",
      "Iteration 2\n",
      "  P1 dist: [2 1 2]\n",
      "  P2 dist: [2 2 1]\n",
      "Iteration 5\n",
      "  P1 dist: [2 4 2]\n",
      "  P2 dist: [3 2 3]\n",
      "Iteration 10\n",
      "  P1 dist: [4 4 5]\n",
      "  P2 dist: [5 5 3]\n",
      "Iteration 20\n",
      "  P1 dist: [7 9 7]\n",
      "  P2 dist: [8 7 8]\n",
      "Iteration 50\n",
      "  P1 dist: [16 19 18]\n",
      "  P2 dist: [19 17 17]\n",
      "Iteration 100\n",
      "  P1 dist: [34 30 39]\n",
      "  P2 dist: [35 36 32]\n",
      "Iteration 200\n",
      "  P1 dist: [65 68 70]\n",
      "  P2 dist: [72 68 63]\n",
      "Iteration 500\n",
      "  P1 dist: [165 164 174]\n",
      "  P2 dist: [172 170 161]\n",
      "Iteration 1000\n",
      "  P1 dist: [328 334 341]\n",
      "  P2 dist: [349 333 321]\n",
      "Iteration 2000\n",
      "  P1 dist: [658 666 679]\n",
      "  P2 dist: [686 667 650]\n",
      "Iteration 5000\n",
      "  P1 dist: [1638 1698 1667]\n",
      "  P2 dist: [1683 1653 1667]\n",
      "Iteration 10000\n",
      "  P1 dist: [3375 3337 3291]\n",
      "  P2 dist: [3312 3336 3355]\n"
     ]
    }
   ],
   "source": [
    "def RPS_best_response(strategy):\n",
    "    r, p, s = strategy\n",
    "    # payoff of playing rock/paper/scissors against opponent mix\n",
    "    return np.argmax([s - p, r - s, p - r])\n",
    "\n",
    "player1 = np.array([1., 1., 1.])\n",
    "player2 = np.array([1., 1., 1.])\n",
    "\n",
    "steps = [1, 2, 5, 10, 20, 50, 100, 200, 500, \n",
    "         1000, 2000, 5000, 10000]  # log / spaced checkpoints\n",
    "\n",
    "for t in range(1, max(steps) + 1):\n",
    "    # best response dynamics\n",
    "    p2_strat = player2 / player2.sum()\n",
    "    player1[RPS_best_response(p2_strat)] += 1\n",
    "\n",
    "    p1_strat = player1 / player1.sum()\n",
    "    player2[RPS_best_response(p1_strat)] += 1\n",
    "\n",
    "    if t in steps:\n",
    "        print(f\"Iteration {t}\")\n",
    "        print(\"  P1 dist:\", player1.astype(int))\n",
    "        print(\"  P2 dist:\", player2.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f7b334-6221-41dd-98c9-a781df6dba14",
   "metadata": {},
   "source": [
    "Over time this converges to a uniform strategy for both players, as you would expect. There are a couple of subtleties to this. Firstly, the outcome can depend slightly on how you break ties, and secondly it can highly depend on the starting distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4575b27f-4f50-45a8-aff0-ebbf983d9120",
   "metadata": {},
   "source": [
    "A steady state in Fictitious play is an action profile for both players that doesn't change from one iteration to the next. I.e., a fixed solution. \n",
    "\n",
    "A couple nice properties of steady states:\n",
    "\n",
    "1. If the game has a pure-strategy nash equilibrium, that pure strategy will be a steady state for fictitious play of the repeated game.\n",
    "2. If a pure-strategy is a steady state of ficitious play in the repeated game then it is a Nash equilibrium in the original game.\n",
    "\n",
    "We can also say in addition to the above that if the empirical distribution converges then it converges to a Nash equilibrium. An example of this is the above Paper-Scissors-Rock game, where there is no steady-state, but the empirical distribution does converge.\n",
    "\n",
    "There are several games where the this property is guaranteed:\n",
    "1. Zero-sum games\n",
    "2. Games which are solvable by iterated elimination of strictly-dominated strategies\n",
    "3. Potential games (or, it seems, any game where the players have identical interests)\n",
    "4. 2xn games with generic payoffs (look it up).\n",
    "\n",
    "Fictitious play is very simple, but limited. We will see more complicated alternatives next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
