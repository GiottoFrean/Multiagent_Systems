{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "605cd606-ff29-465e-9160-8d0c2e73b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034446f5-0475-477c-8975-444e621e6c28",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 10.3 Quasilinear preferences\n",
    "\n",
    "If we relax some requirements we can make some headway in designing dominant-strategy truthful mechanisms. A quasilinear utility function is where the outcomes $x$ are paired up with some payments $p$ and the utility of each agent $i$ is just $u(x,\\theta) - f_i(p_i)$, where $f_i$ is an increasing function. While different plots can exist for $f_i$ (e.g., risk averse), for now we assume it's linear and the same across all people. In fact, to make this really simple we can just say the coefficient is 1. Therefore, utility is comparible to dollars. \n",
    "\n",
    "Under this setting a mechanism is just the set of actions the agents have, the payment function, and the probability of outcomes. We can also assume that the utility between two agents is the same given their type is the same. \n",
    "\n",
    "A truthful mechanism is one in which agents are asked to state their valuations for different outcomes, and the valuations are exactly the utilities|type. Note that this is before any payments are made. We want a mechanism that is truthful and efficient. Efficient means it selects the outcome that maximizes the true utilities of the outcomes (before payments). We also probably want a mechanism to be budget balanced, which is where the sum of the payments is less than 0, i.e., it doesn't make a loss.\n",
    "\n",
    "A mechanism is ex-interim rational if the agent would choose to participate in it (*expects* to gain something). A mechanism is ex-post rational if no agent ever loses by participating in it. \n",
    "\n",
    "We can also have different aims for the mechanism besides maximizing the social wellfare (choosing the best outcome according to the players true valuations). We can also:\n",
    "1. ask that it maximizes or minimizes revenue at equilibrium\n",
    "2. that it does some maxmin fairness and ensures the least-happy player is the most happy they can be.\n",
    "3. minimize the outcome when the worst equilibrium is selected.\n",
    "\n",
    "Etc, etc.\n",
    "\n",
    "Consider this example:\n",
    "\n",
    "$\n",
    "\\begin{array}{c|cc}\n",
    "\\text{} & \\text{C} & \\text{D} \\\\\n",
    "\\hline\n",
    "\\text{C} & a_1,b_1 & a_2,b_2 \\\\\n",
    "\\text{D} & a_2,b_2 & a_2,b_2 \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b2e497-f4d5-411a-8219-d4120236ec79",
   "metadata": {},
   "source": [
    "## 10.4 Efficient mechanisms\n",
    "\n",
    "A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec647b9-c96b-48f0-99a9-c3e73d66087b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa98b38-520c-45f3-a86f-b92839c811b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
